# Machine-Learning
This repository contains all the notebooks I made while learning Machine Learning.
I have used Python 3 in all the projects.<br> I am still working on this repository and I will keep updating it.

# Contents
<ol>
<li><h3><b> Boston housing</b></h3></li>
  <p> I have used scikit-learn's Boston dataset that has a given set of features that describe a house in Boston and our machine learning model must predict the house price. Performed Exploratory Data Analysis on the data and used <u> Linear Regression </u> to predict the target variable. </p>

<li><h3><b> Clustering</b></h3></li>
<p>This folder contains 3 projects.
  <ol type="a">
    <li><h4>K-means from the scratch</li></h4>
    <p> Implemented a hand-coded K-means Clustering algorithm on the 'loan prediction' dataset, considering only 2 features - Loan Amount and Applicant Income. </p>
    <li><h4>Implementation of Hierarchical Clustering</h4></li>
   <p> Implementation of Complete link, Average link, and Ward link Hierarchical Clustering on raw data and normalized data and visualization of Dedrograms</p>
    <li><h4>Implementation of the Gaussian Mixture Models</h4></li>
    <p>Comparison of GMM and K-means clustering on a dataset having weight and height as features, having 4 well-separated clusters.</p>
    </ol></p>
<li><h3><b>Decision Trees</b></h3></li>
<p> This folder contains 3 projects.
  <ol type='i'>
    <li><h4>Visualizing the Decision Boundaries of a Decision Tree</h4></li>
    <li><h4>Hyperparameter Tuning of Decision Trees</h4></li>
    <li><h4>Visualizing a Decision Tree using Graphviz</h4></li>
   </ol>
 </p>
 
<li><h3><b>Evaluation Metrics</li></h3></b>
<p>Implementation of Evaluation Metrics like Confusion Matrix, Accuracy, Precision, Recall, F1-Score from Scratch and comparing the results with sklearn built-in functions for the same metrics.</p>

<li><h3><b>Logistic Regression</b></h3></li>
<p>Here, I have used the Iris dataset considering two 2 features and only 2 target variable to make the data linearly separable. I have hand-coded Logistics Regression algorithm and fitted on the dataset.</p>
<li><h3><b>Naive Bayes</li></h3></b>
<p> Here, I have used Pima Indian Diabetes Dataset. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. I have performed Exploratory Data Analysis on that dataset and fitted the Naive Bayes estimator using Scikit Learn to predict the target variable. </p>

<li><h3><b>PCA</li></h3></b>
<p>Here, I have used the 'MNIST' dataset which comes with handwritten numbers and labels. I have hand-coded the PCA algorithm and used it to visualize the dataset in 2D. I have used scikit learn's implementation for the same to compare the results.

<li><h3><b>Regression</li></h3></b>
<p>Here, I have used the "Big mart Sales" dataset which has been cleaned beforehand. I have fitted Linear regression on the dataset to predict the target variable. I have also checked for the various assumptions that Linear Regression holds, to verify the suitability of applying Linear regression to the dataset.</p>

<li><h3><b>SVM</li></h3></b>
<p>Here, I have used the Iris dataset, considering only 2 features - Sepal Length and Sepal Width. I have visualized decision boundaries for different hyperparameters.</p>

<li><h3><b>Spam or Ham project</li></h3></b>
<p>Here, I have used the SMS Spam Collection Dataset. It contains the text of 5572 SMS messages and a label, classifying the message as "spam" or "ham". I have implemented Bag of Words from scratch and then I have used scikit learn's implementation of Bag of Words to transform the entire dataset. Further, I have used the Naive Bayes algorithm to classify the messages into "spam" or "ham".</p>

<li><h3><b>Supervised Learning Project</li></h3></b>
<p>Here, The is data collected from the U.S. census, to help CharityML (a fictitious charity organization) identify people who are most likely to donate to their cause. Here, we have to predict that if the income of a row is '<=50' or '>50'. I have used 9 different algorithms to check which performs the best on the data and then tuned it according to the dataset.</p>
  
  <li><h3><b>Titanic Dataset</li></h3></b>



